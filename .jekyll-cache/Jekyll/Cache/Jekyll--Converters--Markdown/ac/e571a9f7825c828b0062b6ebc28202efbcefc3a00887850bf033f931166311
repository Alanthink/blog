I"©<h2 id="problem">Problem</h2>

<p>Suppose we have a sequence of <em>i.i.d.</em> Gaussian random variables $X_t$â€™s with bounded variance. Let $S_n = \sum_{t = 1}^n X_t$. How can we bound the probability of</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:problem}
\left\{\max_{1 \leq t \leq n} S_t > \epsilon \right\}? 
\end{equation}</script>

<h2 id="sub-gaussian-random-variables">Sub-gaussian random variables</h2>

<p>To make this problem more realistic, it is always safe to loose Gaussian random variables to zero mean <em>sub-gaussian</em> random variables.</p>

<p>Intuitively, sub-gaussian r.v.â€™s have tails decreasing as fast as those of Gaussian r.v.â€™s. Hence, most of the inequalities related to Gaussian r.v.â€™s can be safely applied to sub-gaussians without any modifications! You are suggested to refer to <a href="http://www.stat.cmu.edu/~arinaldo/36788/subgaussians.pdf">Subgaussian random variables: An expository note</a> for more details. Here, we assume $X_t$ is a $\sigma$-subgaussian r.v. which is comparable to a Gaussian r.v. with variance $\sigma^2$.</p>

<p>An elementary property of $\sigma$-subgaussian random variable $X$ is</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:ele-prop-subgaussian}
\mathbb{E}[ \exp( \lambda X ) ] \leq \exp( \lambda^2 \sigma^2  / 2 ).
\end{equation}</script>

<p>Also, it is useful to know the following facts:</p>

<ul>
  <li>$S_t$ is $\sqrt{t} \sigma$-subgaussian,</li>
  <li>and $\Pr( X &gt; \epsilon ) \leq \exp\left( - \frac{ \epsilon^2 }{2\sigma^2} \right)$.</li>
</ul>

<h2 id="a-naive-way">A naive way</h2>

<p>Obviously, we can use a union bound to give a naive bound.</p>

<p>For each $t$, since $S_t$ is $\sqrt{t} \sigma$-subgaussian we have $\Pr( S_t &gt; \epsilon ) \leq \exp\left( - \frac{\epsilon^2}{2 t \sigma^2} \right)$. Via a union bound, we get</p>

<script type="math/tex; mode=display">\begin{equation*}
\Pr\left( \max_{1 \leq t \leq n} S_t > \epsilon \right) \leq \sum_{t = 1}^n \exp\left( - \frac{\epsilon^2}{2 t \sigma^2 } \right),
\end{equation*}</script>

<p>from which we can see the upper bound is no less then</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:union-bound}
n \exp\left( - \frac{\epsilon^2}{2 n \sigma^2 } \right).
\end{equation}</script>

<p>This bound is very loose. Later, you will see the reason.</p>

<h2 id="an-alternative-way">An alternative way</h2>

<h3 id="doobs-martingale-inequality">Doobâ€™s martingale inequality</h3>

<p>The formal statement of Doobâ€™s martingale inequality can be found in <a href="#doob-inequality">[1]</a>. We restate it in the following.</p>

<p>Suppose the sequence $T_1, \dots, T_n$ is a submartingale, taking non-negative values. Then it holds that</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:doob-inequality}
\Pr\left( \max_{1\leq t \leq n} T_t > \epsilon \right) \leq \frac{ \mathbb{E}[T_n] }{\epsilon}. 
\end{equation}</script>

<p>With this tool in mind, we are now ready to bound $\eqref{eq:problem}$ in another way.</p>

<p>Using standard Chernoffâ€™s method, for any $\lambda &gt; 0$, we have</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\Pr\left( \max_{1 \leq t \leq n} S_t > \epsilon \right) & = \Pr\left( \max_{1 \leq t \leq n} \exp(\lambda S_t) > \exp( \lambda \epsilon) \right ).
\end{align*} %]]></script>

<p>Since $\mathbb{E}[ \exp( \lambda X_t ) ] \geq \exp( \mathbb{E}[ \lambda X_t )] = 1$, sequence $ \exp(\lambda S_1), \dots, \exp(\lambda S_t)$ is a submartingale. (This is a good exercise. You can validate it by yourself.) By $\eqref{eq:doob-inequality}$, we further have</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\Pr\left( \max_{1 \leq t \leq n} S_t > \epsilon \right) & \leq \frac{ \mathbb{E}[\exp(\lambda S_n )] }{ \exp(\lambda \epsilon) } \\
& = \frac{ \prod_{t = 1}^n \mathbb{E}[ \exp(\lambda X_t )] }{ \exp(\lambda \epsilon) } \\
& \leq \exp\left( \frac{\lambda^2 \sigma^2 n}{2} - \lambda \epsilon \right),
\end{align*} %]]></script>

<p>where the second equality is due to the mutual indenpendency of $X_t$â€™s, and the last inequality is due to $\eqref{eq:ele-prop-subgaussian}$.</p>

<p>The minimum is achieved when $\lambda = \frac{\epsilon}{ \sigma^2 n}$. So we finally get</p>

<script type="math/tex; mode=display">\Pr\left( \max_{1 \leq t \leq n} S_t > \epsilon \right) \leq \exp\left( - \frac{\epsilon^2}{2 n \sigma^2 } \right),</script>

<p>which is only one $n$-th of $\eqref{eq:union-bound}$!</p>

<p>Note that Lemma 2 in <a href="#lemma-2">[2]</a> gives the same statement. However, I think it is more direct to use Doobâ€™s martingale inequality.</p>

<h2 id="references">References</h2>

<ol>
  <li>
    <p><a name="doob-inequality"></a> <a href="https://en.wikipedia.org/wiki/Doob%27s_martingale_inequality">Doobâ€™s martingale inequality</a>, Wikipedia.</p>
  </li>
  <li>
    <p><a name="lemma-2"></a> Shengjia Zhao, Enze Zhou, Ashish Sabharwal, and Stefano Ermon. <a href="https://papers.nips.cc/paper/6493-adaptive-concentration-inequalities-for-sequential-decision-problems">Adaptive Concentration Inequalities for Sequential Decision Problems
</a>. In Advances in Neural Information Processing Systems, pages 1343â€“1351, 2016.</p>
  </li>
</ol>
:ET